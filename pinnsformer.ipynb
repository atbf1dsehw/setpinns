{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all the imports\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import LBFGS, AdamW\n",
    "from tqdm import tqdm\n",
    "from src.util import *\n",
    "from src.model.pinnsformer import PINNsformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.set_warn_always(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xavier init and functions to get the analytical sol\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "def h(x):\n",
    "    return np.exp(-((x - np.pi) ** 2) / (2 * (np.pi / 4) ** 2))\n",
    "\n",
    "\n",
    "def u_ana(x, t):\n",
    "    return h(x) * np.exp(5 * t) / (h(x) * np.exp(5 * t) + 1 - h(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "res_points = 50\n",
    "test_points = 101\n",
    "val_points = 21\n",
    "num_step=5\n",
    "step=1e-4\n",
    "device = \"cuda:0\"\n",
    "\n",
    "res, b_left, b_right, b_upper, b_lower = get_data(\n",
    "    [0, 2 * np.pi], [0, 1], res_points, res_points\n",
    ")\n",
    "res_test, _, _, _, _ = get_data([0, 2 * np.pi], [0, 1], test_points, test_points)\n",
    "# val\n",
    "res_val, _, _, _, _ = get_data([0, 2 * np.pi], [0, 1], val_points, val_points)\n",
    "u = torch.from_numpy(u_ana(res_val[:, 0], res_val[:, 1])).reshape(-1).to(device)\n",
    "res_val = make_time_sequence(res_val, num_step=num_step, step=step)\n",
    "res_val = torch.tensor(res_val, dtype=torch.float32, requires_grad=True).to(device)\n",
    "x_val, t_val = res_val[:, :, 0:1], res_val[:, :, 1:2]\n",
    "\n",
    "res = make_time_sequence(res, num_step=num_step, step=step)\n",
    "b_left = make_time_sequence(b_left, num_step=num_step, step=step)\n",
    "b_right = make_time_sequence(b_right, num_step=num_step, step=step)\n",
    "b_upper = make_time_sequence(b_upper, num_step=num_step, step=step)\n",
    "b_lower = make_time_sequence(b_lower, num_step=num_step, step=step)\n",
    "\n",
    "res = torch.tensor(res, dtype=torch.float32, requires_grad=True).to(device)\n",
    "b_left = torch.tensor(b_left, dtype=torch.float32, requires_grad=True).to(device)\n",
    "b_right = torch.tensor(b_right, dtype=torch.float32, requires_grad=True).to(device)\n",
    "b_upper = torch.tensor(b_upper, dtype=torch.float32, requires_grad=True).to(device)\n",
    "b_lower = torch.tensor(b_lower, dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "x_res, t_res = res[:, :, 0:1], res[:, :, 1:2]\n",
    "x_left, t_left = b_left[:, :, 0:1], b_left[:, :, 1:2]\n",
    "x_right, t_right = b_right[:, :, 0:1], b_right[:, :, 1:2]\n",
    "x_upper, t_upper = b_upper[:, :, 0:1], b_upper[:, :, 1:2]\n",
    "x_lower, t_lower = b_lower[:, :, 0:1], b_lower[:, :, 1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now initialize the model and few important variables. One can use any variant of the Tranformer model.\n",
    "# define setpinns\n",
    "model = PINNsformer(d_out=1, d_hidden=512, d_model=32, N=1, heads=2).to(device)\n",
    "# apply xavier init\n",
    "model.apply(init_weights)\n",
    "best_val = np.inf\n",
    "loss_track = []\n",
    "val_track = []\n",
    "best_model_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Training\n",
      "Best val reached at 0\n",
      "Best val reached at 1\n",
      "Best val reached at 7\n",
      "Best val reached at 19\n"
     ]
    }
   ],
   "source": [
    "# now perform training with AdamW\n",
    "# adam training\n",
    "optim = AdamW(model.parameters(), lr=3e-4)\n",
    "print(f\"Adam Training\")\n",
    "for i in range(100):\n",
    "    model.train()\n",
    "    pred_res = model(x_res, t_res)\n",
    "    pred_left = model(x_left, t_left)\n",
    "    pred_upper = model(x_upper, t_upper)\n",
    "    pred_lower = model(x_lower, t_lower)\n",
    "    u_t = torch.autograd.grad(\n",
    "        pred_res,\n",
    "        t_res,\n",
    "        grad_outputs=torch.ones_like(pred_res),\n",
    "        retain_graph=True,\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "    loss_res = torch.mean((u_t - 5 * pred_res * (1 - pred_res)) ** 2)\n",
    "    loss_bc = torch.mean((pred_upper - pred_lower) ** 2)\n",
    "    loss_ic = torch.mean(\n",
    "        (\n",
    "            pred_left[:, 0]\n",
    "            - torch.exp(\n",
    "                -((x_left[:, 0] - torch.pi) ** 2) / (2 * (torch.pi / 4) ** 2)\n",
    "            )\n",
    "        )\n",
    "        ** 2\n",
    "    )\n",
    "\n",
    "    loss_track.append([loss_res.item(), loss_bc.item(), loss_ic.item()])\n",
    "\n",
    "    loss = loss_res + loss_bc + loss_ic\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    pred = model(x_val, t_val)[:, 0:1]\n",
    "    pred = pred.reshape(-1)\n",
    "    r = F.mse_loss(pred, u).item()\n",
    "    val_track.append(r)\n",
    "    if r < best_val:\n",
    "        print(f\"Best val reached at {i}\")\n",
    "        best_val = r\n",
    "        best_model_weights = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now tune the model using LBFGS\n",
    "model.load_state_dict(best_model_weights)\n",
    "model.to(device)\n",
    "optim = LBFGS(model.parameters(), line_search_fn=\"strong_wolfe\")\n",
    "for i in range(1000):\n",
    "    model.train()\n",
    "\n",
    "    def closure():\n",
    "        pred_res = model(x_res, t_res)\n",
    "        pred_left = model(x_left, t_left)\n",
    "        pred_upper = model(x_upper, t_upper)\n",
    "        pred_lower = model(x_lower, t_lower)\n",
    "        u_t = torch.autograd.grad(\n",
    "            pred_res,\n",
    "            t_res,\n",
    "            grad_outputs=torch.ones_like(pred_res),\n",
    "            retain_graph=True,\n",
    "            create_graph=True,\n",
    "        )[0]\n",
    "        loss_res = torch.mean((u_t - 5 * pred_res * (1 - pred_res)) ** 2)\n",
    "        loss_bc = torch.mean((pred_upper - pred_lower) ** 2)\n",
    "        loss_ic = torch.mean(\n",
    "            (\n",
    "                pred_left[:, 0]\n",
    "                - torch.exp(\n",
    "                    -((x_left[:, 0] - torch.pi) ** 2) / (2 * (torch.pi / 4) ** 2)\n",
    "                )\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "\n",
    "        loss_track.append([loss_res.item(), loss_bc.item(), loss_ic.item()])\n",
    "\n",
    "        loss = loss_res + loss_bc + loss_ic\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optim.step(closure)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    pred = model(x_val, t_val)[:, 0:1]\n",
    "    pred = pred.reshape(-1)\n",
    "    r = F.mse_loss(pred, u).item()\n",
    "    val_track.append(r)\n",
    "    if r < best_val:\n",
    "        print(f\"Best val reached at {i}\")\n",
    "        best_val = r\n",
    "        best_model_weights = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rRMSE: 0.978569\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(best_model_weights)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "res_test = make_time_sequence(res_test, num_step=num_step, step=step)\n",
    "res_test = torch.tensor(res_test, dtype=torch.float32, requires_grad=True).to(\n",
    "        device\n",
    "    )\n",
    "x_test, t_test = res_test[:, :, 0:1], res_test[:, :, 1:2]\n",
    "with torch.no_grad():\n",
    "    pred = model(x_test, t_test)[:, 0:1]\n",
    "    pred = pred.cpu().detach().numpy()\n",
    "pred = pred.reshape(test_points, test_points)\n",
    "res_test, _, _, _, _ = get_data([0, 2 * np.pi], [0, 1], test_points, test_points)\n",
    "u = u_ana(res_test[:, 0], res_test[:, 1]).reshape(test_points, test_points)\n",
    "\n",
    "rl1 = np.sum(np.abs(u - pred)) / np.sum(np.abs(u))\n",
    "rl2 = np.sqrt(np.sum((u - pred) ** 2) / np.sum(u**2))\n",
    "\n",
    "print(\"rRMSE: {:4f}\".format(rl2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "setpinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
